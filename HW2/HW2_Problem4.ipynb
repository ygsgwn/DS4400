{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d1c0d6-aa03-42f3-9150-44bfefa8bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2597f9-adde-4632-82ab-72506abf45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "drop_cols = ['Unnamed: 0', 'id', 'date', 'zipcode', 'price']\n",
    "\n",
    "X_train = train_df.drop(columns=[c for c in drop_cols if c in train_df.columns])\n",
    "y_train = train_df['price'] / 1000 \n",
    "\n",
    "X_test  = test_df.drop(columns=[c for c in drop_cols if c in test_df.columns])\n",
    "y_test = test_df['price'] / 1000\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439c4b47-0491-49bd-ac8f-825c22388085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p     Train MSE  Train R2      Test MSE     Test R2\n",
      "0  1  57947.526161  0.496709  8.857598e+04    0.468736\n",
      "1  2  54822.665116  0.523849  7.179168e+04    0.569406\n",
      "2  3  53785.194716  0.532860  9.983348e+04    0.401216\n",
      "3  4  52795.774758  0.541453  2.509793e+05   -0.505331\n",
      "4  5  52626.111955  0.542927  2.865728e+07 -170.881541\n"
     ]
    }
   ],
   "source": [
    "def closed_form(X, y):\n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "def get_polynomial_features(X, degree):\n",
    "    return np.hstack([X**i for i in range(1, degree + 1)])\n",
    "\n",
    "poly_results = []\n",
    "\n",
    "for p in range(1, 6):\n",
    "    X_train_poly = get_polynomial_features(X_train[['sqft_living']].values, p)\n",
    "    X_test_poly = get_polynomial_features(X_test[['sqft_living']].values, p)\n",
    "    \n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_scaled = scaler_poly.fit_transform(X_train_poly)\n",
    "    X_test_scaled = scaler_poly.transform(X_test_poly)\n",
    "    \n",
    "    X_train_final = np.hstack([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled])\n",
    "    X_test_final = np.hstack([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled])\n",
    "    \n",
    "    theta_poly = closed_form(X_train_final, y_train)\n",
    "    \n",
    "    y_train_pred = X_train_final @ theta_poly\n",
    "    y_test_pred = X_test_final @ theta_poly\n",
    "    \n",
    "    poly_results.append({\n",
    "        \"p\": p,\n",
    "        \"Train MSE\": mean_squared_error(y_train, y_train_pred),\n",
    "        \"Train R2\": r2_score(y_train, y_train_pred),\n",
    "        \"Test MSE\": mean_squared_error(y_test, y_test_pred),\n",
    "        \"Test R2\": r2_score(y_test, y_test_pred)\n",
    "    })\n",
    "\n",
    "poly_df = pd.DataFrame(poly_results)\n",
    "print(poly_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f20416-8934-4a0b-976b-e21042eaa4c3",
   "metadata": {},
   "source": [
    "As the degree of the polynomial increases, the Train MSE decreases from 57947.53 at degree 1 to 52626.11 at degree 5 and the Train R^2 increases from 0.497 to 0.543. By increasing the degree of the polynomial, it creates more flexible model that fits the training data more closely. ",
    "However, the Test MSE decreases from degree 1(88575.98) to degree 2 (71791.68) hitting its lowest, and the test R^2 reaches its highest value at degree 2(0.569). For degree larger than 2, the Test MSE increases and the test R^2 deteriorates significantly, dropping to -170.88 at degree 5. ",
    "These results are clear demonstration of overfitting, where the model becomes too complex and sensitive to specific noise in the training data. The Test R^2 becomes negative at degree of 4 and 5. This indicates that the model’s predictions are so inaccurate that predictions are farther from the true values than the simple mean would be. In conclusion, the polynomial regression with degree 2 is the most effective for this dataset. It is the best trade-off between bias and variance, achieving highest accuracy(the lowest test MSE and highest test R^2) on unseen data before the model begins to overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
